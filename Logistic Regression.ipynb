{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daaf2b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the necessary libraries required for the algorithm\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7be31168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784)\n"
     ]
    }
   ],
   "source": [
    "#reading the data's given\n",
    "df = pd.read_csv('classification_train.csv')\n",
    "arr = df.to_numpy()\n",
    "x_values = arr[:,2:786]\n",
    "y_values = arr[:,1]\n",
    "#for finding number of training examples and number of features\n",
    "m = len(x_values)\n",
    "n = len(df.columns)-2\n",
    "\n",
    "#to view the csv file\n",
    "df.head()\n",
    "print(x_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa3fccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1221abe50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_values[2998].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41e4ab54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "(10, 30000)\n"
     ]
    }
   ],
   "source": [
    "num_unique = np.unique(y_values)\n",
    "print((num_unique))\n",
    "y_classes = np.zeros((m,len(num_unique)))\n",
    "for j in range(len(num_unique)):\n",
    "    for i in range(m):\n",
    "        if y_values[i] == num_unique[j]:\n",
    "            y_classes[i,j] = 1\n",
    "y_classes = y_classes.T\n",
    "print(y_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ddff7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784)\n"
     ]
    }
   ],
   "source": [
    "#to assign initial values to w and b\n",
    "w_init = np.zeros((len(num_unique),n ))\n",
    "b_init = np.zeros((len(num_unique),1))\n",
    "print(w_init.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c5ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmod function\n",
    "def sigmoid(z):\n",
    "    answer_func = 1/(1+np.exp(-1*(z)))\n",
    "    return answer_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f83e354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to creat function\n",
    "def main_func(x_values,w,b):\n",
    "    func = np.zeros((m))\n",
    "    for i in range(m):\n",
    "        func[i] = sigmoid(np.dot(x_values[i],w)+b)\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6cc7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to creat function\n",
    "def main_func(x_values,w,b):\n",
    "    func = np.zeros((m))\n",
    "    func[i] = sigmoid(np.matmul(x_values,w)+b)\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e30760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find mean\n",
    "def mean(x_values,m,n):\n",
    "    mean = np.zeros(n,)\n",
    "    for j in range(n):\n",
    "        summation = 0\n",
    "        for i in range(m):\n",
    "            summation += x_values[i,j]\n",
    "        mean[j] = summation/m\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8796846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find standard deviation\n",
    "def std_deviation(x_values,mean,m,n):\n",
    "    summation = np.zeros(n,)\n",
    "    standard_deviation = np.zeros(n,)\n",
    "    for j in range(n):\n",
    "        for i in range(m):\n",
    "            summation[j] += ((x_values[i,j]-mean[j])**2)\n",
    "        summation[j] = (summation[j]/m)\n",
    "        standard_deviation[j] = math.sqrt(summation[j])\n",
    "    return standard_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23a7747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing the z score normalization\n",
    "def z_score(x_values,m,n):\n",
    "    avg = mean(x_values,m,n)\n",
    "    standard_deviation = std_deviation(x_values,avg,m,n)\n",
    "    x_modified = np.zeros_like(x_values)\n",
    "    for j in range(n):\n",
    "        for i in range(m):\n",
    "            x_modified[i,j]= ((x_values[i,j]-avg[j])/standard_deviation[j])\n",
    "    return x_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cc8bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find the loss\n",
    "def cost_func(x_values,y_values,m,w,b):\n",
    "    y_predicted = main_func(x_values,w,b)\n",
    "    loss = (-1/m)*np.sum(y_values*np.log(y_predicted) + (1-y_values)*np.log(1-y_predicted))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "868fcf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the derivative of w and b \n",
    "def derivative(x_values,y_values,m,n,w,b):\n",
    "    y_predicted = main_func(x_values,w,b)\n",
    "    loss = y_values-y_predicted\n",
    "    derivative_w = np.zeros(n)\n",
    "    derivative_b = 0\n",
    "    for i in range(n):\n",
    "        derivative_w[i] = (1/m)*np.sum((loss)*x_values[:,i])\n",
    "    derivative_b = (1/m)*np.sum(loss)\n",
    "    \n",
    "    return derivative_w,derivative_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9487bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cost_func(x_values,y_classes[0],m,w_init[0],b_init[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65815dde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#x_modified = z_score(x_values,m,n)\n",
    "#print(derivative(x_modified,y_classes[0],m,n,w_init[0],b_init[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3fb6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiant_decent(x_values,y_values,m,n,w,b,alpha,num_iterations):\n",
    "    w_temp = copy.deepcopy(w)\n",
    "    b_temp = b\n",
    "    loss_arr = []\n",
    "    for i in range(len(num_unique)):\n",
    "        #running loop of 'no. of iterations times':\n",
    "        print(\"This is for the label : \"+str(i))\n",
    "        for j in range(num_iterations):\n",
    "            #finding derivative of w and b\n",
    "            derivative_w,derivative_b = derivative(x_values,y_values[i],m,n,w_temp[i],b_temp[i])\n",
    "            #changing the values of w and b\n",
    "            for k in range(n):\n",
    "                w_temp[i,k] += alpha*derivative_w[k]\n",
    "            b_temp[i] += alpha*derivative_b\n",
    "            #printing the loss\n",
    "            if(j%50==0):\n",
    "                loss = cost_func(x_values,y_values[i],m,w_temp[i],b_temp[i])\n",
    "                print(\"The loss after \"+str(j)+\" iterations is \"+str(loss))\n",
    "    return w_temp,b_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c55bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x_values,y_values,m,n,w,b,alpha,num_iterations):\n",
    "    w_tmp = np.zeros_like(w_init)\n",
    "    b_tmp = np.zeros_like(b_init)\n",
    "    w_tmp,b_tmp = gradiant_decent(x_values,y_values,m,n,w,b,alpha,num_iterations)\n",
    "    print(\"The values of w and b are \"+str(w_tmp)+\" \"+str(b_tmp))\n",
    "    return w_tmp,b_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "696d2b23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is for the label : 0\n",
      "The loss after 0 iterations is 0.31962484030972527\n",
      "The loss after 50 iterations is 0.14186141493279772\n",
      "The loss after 100 iterations is 0.13041197751094372\n",
      "The loss after 150 iterations is 0.12551521102309385\n",
      "The loss after 200 iterations is 0.12254923932294896\n",
      "The loss after 250 iterations is 0.1204527390941658\n",
      "The loss after 300 iterations is 0.11883976991491371\n",
      "The loss after 350 iterations is 0.11753188044748451\n",
      "The loss after 400 iterations is 0.11643326864316135\n",
      "The loss after 450 iterations is 0.11548696014373722\n",
      "The loss after 500 iterations is 0.11465644320639333\n",
      "The loss after 550 iterations is 0.11391695087457752\n",
      "The loss after 600 iterations is 0.11325091755642226\n",
      "The loss after 650 iterations is 0.11264543176515307\n",
      "The loss after 700 iterations is 0.11209072166972774\n",
      "The loss after 750 iterations is 0.1115792102373695\n",
      "The loss after 800 iterations is 0.11110490147475664\n",
      "The loss after 850 iterations is 0.11066296779696083\n",
      "The loss after 900 iterations is 0.11024946423380859\n",
      "The loss after 950 iterations is 0.10986112526460537\n",
      "The loss after 1000 iterations is 0.10949521705567332\n",
      "This is for the label : 1\n",
      "The loss after 0 iterations is 0.23505240682778744\n",
      "The loss after 50 iterations is 0.06398847009476522\n",
      "The loss after 100 iterations is 0.05023222580409237\n",
      "The loss after 150 iterations is 0.04441021076105728\n",
      "The loss after 200 iterations is 0.04107255269162302\n",
      "The loss after 250 iterations is 0.03885600042707061\n",
      "The loss after 300 iterations is 0.03724947313644582\n",
      "The loss after 350 iterations is 0.036015098241893265\n",
      "The loss after 400 iterations is 0.03502606218523441\n",
      "The loss after 450 iterations is 0.034208144559325034\n",
      "The loss after 500 iterations is 0.03351483843599137\n",
      "The loss after 550 iterations is 0.0329154413130702\n",
      "The loss after 600 iterations is 0.03238882952371243\n",
      "The loss after 650 iterations is 0.031919972695038046\n",
      "The loss after 700 iterations is 0.03149787207409876\n",
      "The loss after 750 iterations is 0.031114284923232274\n",
      "The loss after 800 iterations is 0.030762905090227232\n",
      "The loss after 850 iterations is 0.030438819640029324\n",
      "The loss after 900 iterations is 0.030138138623005815\n",
      "The loss after 950 iterations is 0.029857736822287574\n",
      "The loss after 1000 iterations is 0.029595069890327348\n",
      "This is for the label : 2\n",
      "The loss after 0 iterations is 0.31756300024880046\n",
      "The loss after 50 iterations is 0.18655600579352022\n",
      "The loss after 100 iterations is 0.17384605377935314\n",
      "The loss after 150 iterations is 0.16767755442258633\n",
      "The loss after 200 iterations is 0.1637995498939002\n",
      "The loss after 250 iterations is 0.1610321432125735\n",
      "The loss after 300 iterations is 0.15890401252062641\n",
      "The loss after 350 iterations is 0.15718668557994742\n",
      "The loss after 400 iterations is 0.1557543967855782\n",
      "The loss after 450 iterations is 0.1545311816037934\n",
      "The loss after 500 iterations is 0.15346779265996477\n",
      "The loss after 550 iterations is 0.15253050734947474\n",
      "The loss after 600 iterations is 0.15169521808288422\n",
      "The loss after 650 iterations is 0.1509440860517879\n",
      "The loss after 700 iterations is 0.15026353463950182\n",
      "The loss after 750 iterations is 0.14964298754035482\n",
      "The loss after 800 iterations is 0.14907404290106496\n",
      "The loss after 850 iterations is 0.14854991422218536\n",
      "The loss after 900 iterations is 0.14806504072265883\n",
      "The loss after 950 iterations is 0.1476148089150283\n",
      "The loss after 1000 iterations is 0.1471953492583173\n",
      "This is for the label : 3\n",
      "The loss after 0 iterations is 0.2779151736703258\n",
      "The loss after 50 iterations is 0.13433158267280437\n",
      "The loss after 100 iterations is 0.12110538386039553\n",
      "The loss after 150 iterations is 0.11493454376591025\n",
      "The loss after 200 iterations is 0.11114518770876727\n",
      "The loss after 250 iterations is 0.10848842337512203\n",
      "The loss after 300 iterations is 0.10647004369555893\n",
      "The loss after 350 iterations is 0.10485225411752863\n",
      "The loss after 400 iterations is 0.10350558886328916\n",
      "The loss after 450 iterations is 0.1023531589336858\n",
      "The loss after 500 iterations is 0.10134622969998282\n",
      "The loss after 550 iterations is 0.10045229980141901\n",
      "The loss after 600 iterations is 0.09964878223761671\n",
      "The loss after 650 iterations is 0.09891942423440628\n",
      "The loss after 700 iterations is 0.09825215997971909\n",
      "The loss after 750 iterations is 0.09763776006327916\n",
      "The loss after 800 iterations is 0.09706895642976554\n",
      "The loss after 850 iterations is 0.09653987433807903\n",
      "The loss after 900 iterations is 0.09604567005778936\n",
      "The loss after 950 iterations is 0.09558229973230564\n",
      "The loss after 1000 iterations is 0.09514636295149104\n",
      "This is for the label : 4\n",
      "The loss after 0 iterations is 0.29674548583481386\n",
      "The loss after 50 iterations is 0.16287105086926798\n",
      "The loss after 100 iterations is 0.15270156019463382\n",
      "The loss after 150 iterations is 0.14822245027778325\n",
      "The loss after 200 iterations is 0.14553974739695266\n",
      "The loss after 250 iterations is 0.1436608891860032\n",
      "The loss after 300 iterations is 0.14221996905930617\n",
      "The loss after 350 iterations is 0.1410509820773034\n",
      "The loss after 400 iterations is 0.14006714197997763\n",
      "The loss after 450 iterations is 0.13921801851830878\n",
      "The loss after 500 iterations is 0.13847179821581712\n",
      "The loss after 550 iterations is 0.1378070660096371\n",
      "The loss after 600 iterations is 0.13720864505160063\n",
      "The loss after 650 iterations is 0.13666533134889855\n",
      "The loss after 700 iterations is 0.13616858108014473\n",
      "The loss after 750 iterations is 0.13571170777960312\n",
      "The loss after 800 iterations is 0.1352893676871768\n",
      "The loss after 850 iterations is 0.13489721605897637\n",
      "The loss after 900 iterations is 0.13453166941550312\n",
      "The loss after 950 iterations is 0.13418973604304912\n",
      "The loss after 1000 iterations is 0.13386889201677593\n",
      "This is for the label : 5\n",
      "The loss after 0 iterations is 0.23796375315781507\n",
      "The loss after 50 iterations is 0.12290635471082938\n",
      "The loss after 100 iterations is 0.10488980211025481\n",
      "The loss after 150 iterations is 0.09578622421709594\n",
      "The loss after 200 iterations is 0.08998330075194363\n",
      "The loss after 250 iterations is 0.085833147215841\n",
      "The loss after 300 iterations is 0.0826526805105299\n",
      "The loss after 350 iterations is 0.0801005016586772\n",
      "The loss after 400 iterations is 0.07798420939745011\n",
      "The loss after 450 iterations is 0.07618584395153286\n",
      "The loss after 500 iterations is 0.07462841023234575\n",
      "The loss after 550 iterations is 0.07325915301500494\n",
      "The loss after 600 iterations is 0.07204049808708347\n",
      "The loss after 650 iterations is 0.07094482317266099\n",
      "The loss after 700 iterations is 0.06995127979549619\n",
      "The loss after 750 iterations is 0.06904377745590207\n",
      "The loss after 800 iterations is 0.06820965830052805\n",
      "The loss after 850 iterations is 0.06743879866640229\n",
      "The loss after 900 iterations is 0.06672298368732155\n",
      "The loss after 950 iterations is 0.06605546181050154\n",
      "The loss after 1000 iterations is 0.06543062095072416\n",
      "This is for the label : 6\n",
      "The loss after 0 iterations is 0.34496353229975146\n",
      "The loss after 50 iterations is 0.23932435518533726\n",
      "The loss after 100 iterations is 0.2252978313410961\n",
      "The loss after 150 iterations is 0.21714155524708345\n",
      "The loss after 200 iterations is 0.21174932926467943\n",
      "The loss after 250 iterations is 0.20791507591491198\n",
      "The loss after 300 iterations is 0.20504445147591846\n",
      "The loss after 350 iterations is 0.2028095800821207\n",
      "The loss after 400 iterations is 0.2010152920352197\n",
      "The loss after 450 iterations is 0.1995385600790263\n",
      "The loss after 500 iterations is 0.19829820975474313\n",
      "The loss after 550 iterations is 0.19723862974902262\n",
      "The loss after 600 iterations is 0.19632049760816742\n",
      "The loss after 650 iterations is 0.19551524701897827\n",
      "The loss after 700 iterations is 0.19480163477738668\n",
      "The loss after 750 iterations is 0.19416353754396667\n",
      "The loss after 800 iterations is 0.1935884957644719\n",
      "The loss after 850 iterations is 0.19306672622511298\n",
      "The loss after 900 iterations is 0.19259043689881195\n",
      "The loss after 950 iterations is 0.1921533417046774\n",
      "The loss after 1000 iterations is 0.19175031046440497\n",
      "This is for the label : 7\n",
      "The loss after 0 iterations is 0.25803574583184485\n",
      "The loss after 50 iterations is 0.09321494688895497\n",
      "The loss after 100 iterations is 0.08100197055707159\n",
      "The loss after 150 iterations is 0.07475398757572393\n",
      "The loss after 200 iterations is 0.07067754438471197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 250 iterations is 0.067725958711866\n",
      "The loss after 300 iterations is 0.06545597956723512\n",
      "The loss after 350 iterations is 0.0636381371253198\n",
      "The loss after 400 iterations is 0.0621388910678127\n",
      "The loss after 450 iterations is 0.06087424593557396\n",
      "The loss after 500 iterations is 0.05978830360812831\n",
      "The loss after 550 iterations is 0.05884221440211676\n",
      "The loss after 600 iterations is 0.05800802339535348\n",
      "The loss after 650 iterations is 0.05726503258229365\n",
      "The loss after 700 iterations is 0.056597545527969154\n",
      "The loss after 750 iterations is 0.05599341330887686\n",
      "The loss after 800 iterations is 0.055443065661181214\n",
      "The loss after 850 iterations is 0.054938846965140745\n",
      "The loss after 900 iterations is 0.054474549884951574\n",
      "The loss after 950 iterations is 0.0540450807277745\n",
      "The loss after 1000 iterations is 0.05364621471847939\n",
      "This is for the label : 8\n",
      "The loss after 0 iterations is 0.26331904804733514\n",
      "The loss after 50 iterations is 0.09179078932806525\n",
      "The loss after 100 iterations is 0.07769670245947415\n",
      "The loss after 150 iterations is 0.07133867119678672\n",
      "The loss after 200 iterations is 0.06744763242800268\n",
      "The loss after 250 iterations is 0.0647156726894385\n",
      "The loss after 300 iterations is 0.06264116539500021\n",
      "The loss after 350 iterations is 0.0609840329688952\n",
      "The loss after 400 iterations is 0.05961265602809246\n",
      "The loss after 450 iterations is 0.05844786486984884\n",
      "The loss after 500 iterations is 0.05743869473388653\n",
      "The loss after 550 iterations is 0.056550582700316834\n",
      "The loss after 600 iterations is 0.05575909868803685\n",
      "The loss after 650 iterations is 0.05504638351144085\n",
      "The loss after 700 iterations is 0.054399012540054546\n",
      "The loss after 750 iterations is 0.05380665578654682\n",
      "The loss after 800 iterations is 0.05326120509907699\n",
      "The loss after 850 iterations is 0.052756186814554275\n",
      "The loss after 900 iterations is 0.052286355183964205\n",
      "The loss after 950 iterations is 0.05184740390688604\n",
      "The loss after 1000 iterations is 0.051435756997957174\n",
      "This is for the label : 9\n",
      "The loss after 0 iterations is 0.24939198237600796\n",
      "The loss after 50 iterations is 0.07031863475867398\n",
      "The loss after 100 iterations is 0.06198912174051986\n",
      "The loss after 150 iterations is 0.05790299322300119\n",
      "The loss after 200 iterations is 0.055177797573818514\n",
      "The loss after 250 iterations is 0.05314024028479451\n",
      "The loss after 300 iterations is 0.05152443407301368\n",
      "The loss after 350 iterations is 0.05019460339143768\n",
      "The loss after 400 iterations is 0.0490709644455805\n",
      "The loss after 450 iterations is 0.04810247777642519\n",
      "The loss after 500 iterations is 0.04725455897799305\n",
      "The loss after 550 iterations is 0.04650275210366669\n",
      "The loss after 600 iterations is 0.04582917195490477\n",
      "The loss after 650 iterations is 0.04522037370649114\n",
      "The loss after 700 iterations is 0.044666014721793774\n",
      "The loss after 750 iterations is 0.04415798101323672\n",
      "The loss after 800 iterations is 0.04368979839337663\n",
      "The loss after 850 iterations is 0.043256224458922146\n",
      "The loss after 900 iterations is 0.04285295901849096\n",
      "The loss after 950 iterations is 0.04247643419957411\n",
      "The loss after 1000 iterations is 0.0421236594362329\n",
      "The values of w and b are [[-0.03182169  0.03488617  0.0325938  ... -0.04031865 -0.03112904\n",
      "   0.01655812]\n",
      " [-0.00509074 -0.01115224 -0.01719997 ... -0.01513584 -0.02232407\n",
      "  -0.00622833]\n",
      " [-0.0303686  -0.05351355  0.03564598 ...  0.04006159  0.03287976\n",
      "   0.0629907 ]\n",
      " ...\n",
      " [-0.0021746  -0.00552033 -0.01977997 ... -0.02311227 -0.01392043\n",
      "  -0.00807   ]\n",
      " [-0.03930728  0.00459027 -0.0914867  ... -0.01315705 -0.0385534\n",
      "  -0.08108774]\n",
      " [-0.01269493  0.01193915 -0.00517906 ...  0.04970905  0.04950866\n",
      "   0.01112337]] [[-1.64043925]\n",
      " [-1.98097103]\n",
      " [-1.39063247]\n",
      " [-1.58131724]\n",
      " [-1.32183719]\n",
      " [-2.9277887 ]\n",
      " [-0.95292151]\n",
      " [-1.63279103]\n",
      " [-2.55112166]\n",
      " [-1.91832419]]\n"
     ]
    }
   ],
   "source": [
    "w_final = np.zeros_like(w_init)\n",
    "b_final = np.zeros_like(b_init)\n",
    "x_modified = z_score(x_values,m,n)\n",
    "w_final,b_final = logistic_regression(x_modified,y_classes,m,n,w_init,b_init,0.1,1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da3da4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_classes_cap \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mzeros_like(y_classes)\n\u001b[1;32m      2\u001b[0m y_cap \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(y_values)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(num_unique)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "y_classes_cap = np.zeros_like(y_classes)\n",
    "y_cap = np.zeros_like(y_values)\n",
    "for i in range(len(num_unique)):\n",
    "    y_classes_cap[i] = main_func(x_modified,w_final[i],b_final[i])\n",
    "for i in range(30000):\n",
    "    temp = np.argmax(y_classes_cap.T[i])\n",
    "    y_cap[i] = num_unique[temp]\n",
    "#for checking accuracy\n",
    "equal = np.sum(y_cap==y_values)\n",
    "accuracy = (equal/m)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9281d096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
